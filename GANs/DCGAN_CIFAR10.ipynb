{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8Jekv-Vdl21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, LeakyReLU, Dropout, Reshape, Conv2DTranspose, BatchNormalization, GaussianDropout\n",
        "from tensorflow.keras.activations import selu\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "from pylab import *\n",
        "rcParams[\"figure.figsize\"] = [8, 8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0mk3MKdfgJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X, _), (_, _) = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECwNHp63foZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = X.shape[-3:]\n",
        "inp, inp[0], inp[1], inp[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bggJ0RW1eMJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykTsnDHNho9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'drive/My Drive/DCGAN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTMgFo4t5Q3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_SMOOTHING = 0\n",
        "D_LR = .0002\n",
        "D_NOISE = False\n",
        "D_BN = False\n",
        "G_BN = False\n",
        "G_GAU_NOISE = False\n",
        "GAN_LR = .0002\n",
        "FLIP = False\n",
        "FLIP_FREQ = 4\n",
        "SELU = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q8r2mXJF_CH",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZxXHxFf8mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_discriminator(in_shape = inp, BN = D_BN, selu = SELU, lr = D_LR, noise = D_NOISE):\n",
        "    model = Sequential()\n",
        "    if selu:\n",
        "        if noise:\n",
        "            model.add(GaussianDropout(.2, input_shape=in_shape))\n",
        "            model.add(Conv2D(64, (3,3), padding='same', activation='selu'))\n",
        "        else:\n",
        "            model.add(Conv2D(64, (3,3), padding='same', activation='selu', input_shape=in_shape))\n",
        "        #NOTE: Consider crippling the discriminator - make the number of filters much less than in the generator.\n",
        "        # downsample\n",
        "        model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', activation='selu'))\n",
        "        # downsample\n",
        "        model.add(Conv2D(256, (3,3), strides=(2,2), padding='same', activation='selu'))\n",
        "        # downsample\n",
        "        model.add(Conv2D(256, (3,3), strides=(2,2), padding='same', activation='selu'))\n",
        "        # classifier\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # compile model\n",
        "        opt = Adam(lr = lr, beta_1 = 0.5)\n",
        "        #opt = SGD(lr = lr, decay = 1e-6)\n",
        "        model.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING), \n",
        "                      optimizer = opt, metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "        if noise:\n",
        "            model.add(GaussianDropout(.2, input_shape=in_shape))\n",
        "            model.add(Conv2D(64, (3,3), padding='same'))\n",
        "        else:\n",
        "            model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "        #NOTE: Crippling the discriminator - make the number of filters much less than in the generator.\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # downsample\n",
        "        model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))   \n",
        "        # downsample\n",
        "        model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # downsample\n",
        "        model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # classifier\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # compile model\n",
        "        opt = Adam(lr = lr, beta_1 = 0.5)\n",
        "        #opt = SGD(lr = lr, decay = 1e-6)\n",
        "        model.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING), \n",
        "                      optimizer = opt, metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "# load and prepare cifar10 training images\n",
        "def load_real_samples():\n",
        "    # load cifar10 dataset\n",
        "    (trainX, _), (_, _) = load_data()\n",
        "    # convert from unsigned ints to floats\n",
        "    X = trainX.astype('float32')\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    X = (X - 127.5) / 127.5\n",
        "    return X\n",
        " \n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "    # choose random instances\n",
        "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "    # retrieve selected images\n",
        "    X = dataset[ix]\n",
        "    # generate 'real' class labels (1)\n",
        "    y = np.ones((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "# generate n fake samples with class labels\n",
        "def fake_samples(n_samples):\n",
        "    # generate uniform random numbers in [0,1]\n",
        "    X = np.random.rand(inp[0] * inp[1] * inp[2] * n_samples)\n",
        "    # update to have the range [-1, 1]\n",
        "    X = -1 + X * 2\n",
        "    # reshape into a batch of color images\n",
        "    X = X.reshape((n_samples, inp[0], inp[1], inp[2]))\n",
        "    # generate 'fake' class labels (0)\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return X, y\n",
        " \n",
        "# train the discriminator model\n",
        "def train_discriminator(model, dataset, n_iter=20, n_batch=128):\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_iter):\n",
        "        # get randomly selected 'real' samples\n",
        "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "        # update discriminator on real samples\n",
        "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
        "        # generate 'fake' examples\n",
        "        X_fake, y_fake = fake_samples(half_batch)\n",
        "        # update discriminator on fake samples\n",
        "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "        # summarize performance\n",
        "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0v3PxV8hSFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the discriminator model\n",
        "discriminator = define_discriminator()\n",
        "discriminator.summary()\n",
        "tf.keras.utils.plot_model(discriminator, to_file=os.path.join(path, 'discriminator_plot.png'), show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HJ5nGVDGdnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# fit the model\n",
        "train_discriminator(discriminator, dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2__x78hGDep",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0lXK2iUGGBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, BN = G_BN, selu = SELU, GauNoise = G_GAU_NOISE):\n",
        "    model = Sequential()\n",
        "    if selu:\n",
        "        # foundation for 4x4 image\n",
        "        n_nodes = 256 * 4 * 4\n",
        "        model.add(Dense(n_nodes, input_dim=latent_dim, activation='selu'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2)) # gaussian noise\n",
        "        model.add(Reshape((4, 4, 256)))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 8x8\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', activation='selu'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 16x16\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', activation='selu'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 32x32\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', activation='selu'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # output layer\n",
        "        model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "\n",
        "        # foundation for 4x4 image\n",
        "        n_nodes = 256 * 4 * 4\n",
        "        model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2)) # gaussian noise \n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(Reshape((4, 4, 256)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 8x8\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2))\n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 16x16\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2))\n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # upsample to 32x32\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        if GauNoise:\n",
        "            model.add(GaussianDropout(.2)) \n",
        "        if BN:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.5))\n",
        "        # output layer\n",
        "        model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "        return model\n",
        "\n",
        " \n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    # generate points in the latent space \n",
        "    # calling the randn() NumPy function for generating arrays of random numbers drawn from a standard Gaussian\n",
        "    x_input = np.random.randn(latent_dim * n_samples)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        " \n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "    # generate points in latent space\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # predict outputs\n",
        "    X = g_model.predict(x_input)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsx6NRlcKc7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# define the discriminator model\n",
        "model = define_generator(latent_dim)\n",
        "# generate samples\n",
        "n_samples = 49\n",
        "X, _ = generate_fake_samples(model, latent_dim, n_samples)\n",
        "# scale pixel values from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the generated samples\n",
        "for i in range(n_samples):\n",
        "    # define subplot\n",
        "    plt.subplot(7, 7, 1 + i)\n",
        "    # turn off axis labels\n",
        "    plt.axis('off')\n",
        "    # plot single image\n",
        "    plt.imshow(X[i])\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY1XZ58oH4u3",
        "colab_type": "text"
      },
      "source": [
        "Note: the generator model is not compiled and does not specify a loss function or optimization algorithm. This is because the generator is not trained directly. We first update the discriminator model with real and fake samples, then update the generator via the composite model (stack). That way the generator is forced to produce more realistic images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ert12tLcHcmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the generator model\n",
        "generator = define_generator(latent_dim)\n",
        "generator.summary()\n",
        "tf.keras.utils.plot_model(generator, to_file=os.path.join(path, 'generator_plot.png'), show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HvSPXv9Nkez",
        "colab_type": "text"
      },
      "source": [
        "## Stack models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XLzDaiYNnji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model, lr = GAN_LR):\n",
        "    # make weights in the discriminator not trainable\n",
        "    d_model.trainable = False\n",
        "    # connect them\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(g_model)\n",
        "    # add the discriminator\n",
        "    model.add(d_model)\n",
        "    # compile model\n",
        "    opt = Adam(lr = lr, beta_1 = 0.5)\n",
        "    model.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING), optimizer = opt)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUF8cArqakGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=7):\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    examples = (examples + 1) / 2.0\n",
        "    # plot images\n",
        "    for i in range(n * n):\n",
        "      # define subplot\n",
        "      plt.subplot(n, n, 1 + i)\n",
        "      # turn off axis\n",
        "      plt.axis('off')\n",
        "      # plot raw pixel data\n",
        "      plt.imshow(examples[i])\n",
        "    # save plot to file\n",
        "    if (epoch+1) % 10 == 0: \n",
        "        filename = os.path.join(path, 'generated_plot_e%03d.png' % (epoch+1))\n",
        "        plt.savefig(filename)\n",
        "        plt.show()\n",
        "    plt.close()\n",
        " \n",
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "    # prepare real samples\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "    # evaluate discriminator on real examples\n",
        "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "    # prepare fake examples\n",
        "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    # evaluate discriminator on fake examples\n",
        "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "    # summarize discriminator performance\n",
        "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "    # save plot\n",
        "    save_plot(x_fake, epoch)\n",
        "\n",
        "def flip_labels (y, pct, n):\n",
        "    # randomly flip labels from 0 to 1 and 1 to 0 for discriminator. Adds noise\n",
        "    idx = np.random.choice(np.arange(y.size), replace=False, size=int(y.size * pct))\n",
        "    y[idx] = n\n",
        "\n",
        "    return y\n",
        " \n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
        "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # enumerate batches over the training set\n",
        "        for j in range(bat_per_epo):\n",
        "            # get randomly selected 'real' samples\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            # flip some of the labels to 0\n",
        "            #y_real = flip_labels(y_real, .1, 0)\n",
        "            # NOTE: Label flipping - train every third batch on flipped labels\n",
        "            if (i+1) % FLIP_FREQ == 0 and FLIP:\n",
        "                y_real = np.full_like(y_real, 0)\n",
        "            # update discriminator model weights\n",
        "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "            # generate 'fake' examples\n",
        "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            # flip some of the labels to 1\n",
        "            #y_fake = flip_labels(y_fake, .1, 0)\n",
        "            if (i+1) % FLIP_FREQ == 0 and FLIP:\n",
        "                y_fake = np.full_like(y_fake, 1)\n",
        "            # update discriminator model weights\n",
        "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "            # prepare points in latent space as input for the generator\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            # create inverted labels for the fake samples\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "            # update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "            # summarize loss on this batch\n",
        "            if (j+1) % bat_per_epo == 0:\n",
        "                print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "      # evaluate the model performance, sometimes\n",
        "        if (i+1) % 1 == 0:\n",
        "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "        if (i+1) % 20 == 0:\n",
        "            # save the generator model file\n",
        "            filename = os.path.join(path, 'generator_model_%03d.h5') % (i+1)\n",
        "            g_model.save(filename)\n",
        " \n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# summarize gan model\n",
        "gan_model.summary()\n",
        "# plot gan model\n",
        "tf.keras.utils.plot_model(gan_model, to_file = os.path.join(path, 'gan_plot.png'), show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9uDJREIyp7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dummy test the stacked model\n",
        "def train_gan(gan_model, latent_dim = 100, n_iter=20, n_batch=128):\n",
        "    for i in range(n_iter):\n",
        "        # get randomly selected 'real' samples\n",
        "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "        # create inverted labels for the fake samples\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # update the generator via the discriminator's error\n",
        "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "        # summarize loss on this batch\n",
        "        print('>%d, g=%.3f' % (i+1, g_loss))\n",
        "\n",
        "train_gan(gan_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpMV5lsgOyGn",
        "colab_type": "text"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCFQ72BnNyVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqSxmnjRlgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}