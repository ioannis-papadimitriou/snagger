{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HZRd5me2Go5"
   },
   "source": [
    "# Overview\n",
    "\n",
    "1. [Keras](#keras)\n",
    "    - [(important!) Solving Scoring Method](#solving-the-problem-with-scoring-method)\n",
    "2. [XGBoost](#xgboost)\n",
    "3. [LightGBM](#lightgbm)\n",
    "4. [Scikit-Learn (sklearn)](#scikit-learn)\n",
    "5. [Extending with Random Search](#randomsearchcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaIBhyaG2zZR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import load_boston, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oO5pj5oR8VTE"
   },
   "outputs": [],
   "source": [
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "      pred = fitted_model.predict_proba(X_test_data)\n",
    "    else:\n",
    "      pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxM4tWsJ8h4s"
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvGefrHe_YQ8"
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# PREPROCESSING\n",
    "def preprocess_mnist(x_train, y_train, x_test, y_test):\n",
    "    # Normalizing all images of 28x28 pixels\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "    # Float values for division\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Normalizing the RGB codes by dividing it to the max RGB value\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    # Categorical y values\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test= to_categorical(y_test, 10)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, input_shape\n",
    "    \n",
    "X_train, y_train, X_test, y_test, input_shape = preprocess_mnist(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tog3I6d38ku0"
   },
   "outputs": [],
   "source": [
    "# Readying neural network model\n",
    "def build_cnn(activation = 'relu',\n",
    "              dropout_rate = 0.2,\n",
    "              optimizer = 'Adam'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "              activation=activation,\n",
    "              input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=optimizer, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "AQLFD9OH_xiq",
    "outputId": "4a1b9645-8e5c-4a54-e82c-cd79ebfd24c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04399333562212302\n",
      "{'batch_size': 128, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "              'epochs':[1,2,3],\n",
    "              'batch_size':[128]\n",
    "              #'epochs' :              [100,150,200],\n",
    "              #'batch_size' :          [32, 128],\n",
    "              #'optimizer' :           ['Adam', 'Nadam'],\n",
    "              #'dropout_rate' :        [0.2, 0.3],\n",
    "              #'activation' :          ['relu', 'elu']\n",
    "             }\n",
    "\n",
    "model = KerasClassifier(build_fn = build_cnn, verbose=0)\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                        param_grid, cv=5, scoring_fit='neg_log_loss')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5a2rfgo1may"
   },
   "source": [
    "## Solving the problem with scoring method\n",
    "We need to remove the categorical encoding of the output datasets (y_train and y_test), for GridSearchCV to work. It has something to do with how scikit-learn converts such variables, which is different from how Keras does it. [Link for mere info.](https://github.com/keras-team/keras/issues/9331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xWixrDnF1lDq",
    "outputId": "b15d65e0-d398-4973-eb48-8c2cb85732e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# PREPROCESSING\n",
    "def preprocess_mnist(x_train, y_train, x_test, y_test):\n",
    "    # Normalizing all images of 28x28 pixels\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "    # Float values for division\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Normalizing the RGB codes by dividing it to the max RGB value\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, input_shape\n",
    "    \n",
    "X_train, y_train, X_test, y_test, input_shape = preprocess_mnist(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "jQp9t9Vb19T3",
    "outputId": "0776e029-9569-4db9-d442-1710a7e029ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "0.9858\n",
      "{'batch_size': 128, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "              'epochs':[1,2,3],\n",
    "              'batch_size':[128]\n",
    "              #'epochs' :              [100,150,200],\n",
    "              #'batch_size' :          [32, 128],\n",
    "              #'optimizer' :           ['Adam', 'Nadam'],\n",
    "              #'dropout_rate' :        [0.2, 0.3],\n",
    "              #'activation' :          ['relu', 'elu']\n",
    "             }\n",
    "\n",
    "model = KerasClassifier(build_fn = build_cnn, verbose=0)\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                        param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOneDewQ8f8u"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3clcGa6P_LPm"
   },
   "outputs": [],
   "source": [
    "# Import boston house prices dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "nXhndZox8ebr",
    "outputId": "b27563e0-b288-40e8-e5f0-81548f25e6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1981 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2430 out of 2430 | elapsed: 15.4min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:43:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "nan\n",
      "{'colsample_bytree': 0.8, 'max_depth': 20, 'n_estimators': 400, 'reg_alpha': 1.2, 'reg_lambda': 1.3, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in sqrt\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'subsample': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rs9V2w1s1eOP",
    "outputId": "45e6a6b9-797d-4d00-de85-4ba3d4a41747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4849014783892733\n",
      "{'colsample_bytree': 0.8, 'max_depth': 20, 'n_estimators': 400, 'reg_alpha': 1.2, 'reg_lambda': 1.3, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80ZKkzgi8Rjj"
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsSpnJ8JE8bZ"
   },
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "UabzOyPU67F5",
    "outputId": "320c98a0-e0d7-48ff-e874-1031e6bb5354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 620 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1432 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2564 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4024 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5804 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7912 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10340 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13096 tasks      | elapsed: 14.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736263736263736\n",
      "{'colsample_bytree': 0.7, 'max_depth': 15, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 50, 'reg_alpha': 1.3, 'reg_lambda': 1.1, 'subsample': 0.7, 'subsample_freq': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 14580 out of 14580 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'num_leaves': [50, 100, 200],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.3, 0.4],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'subsample_freq': [20]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IduhYUQ38wTN"
   },
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq9vDPPE2ycD"
   },
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ZoQ0jVEc8vnr",
    "outputId": "1c6d14c8-cbc2-4c91-9f71-791fbb183024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9648351648351648\n",
      "{'max_depth': 25, 'max_leaf_nodes': 50, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'max_depth': [15,20,25],\n",
    "    'max_leaf_nodes': [50, 100, 200]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DsS_q-bqihq"
   },
   "source": [
    "# RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F704qh7cqf-o"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def search_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False, search_mode = 'GridSearchCV', n_iterations = 0):\n",
    "    fitted_model = None\n",
    "    \n",
    "    if(search_mode == 'GridSearchCV'):\n",
    "        gs = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid, \n",
    "            cv=cv, \n",
    "            n_jobs=-1, \n",
    "            scoring=scoring_fit,\n",
    "            verbose=2\n",
    "        )\n",
    "        fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "\n",
    "    elif (search_mode == 'RandomizedSearchCV'):\n",
    "        rs = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid, \n",
    "            cv=cv,\n",
    "            n_iter=n_iterations,\n",
    "            n_jobs=-1, \n",
    "            scoring=scoring_fit,\n",
    "            verbose=2\n",
    "        )\n",
    "        fitted_model = rs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    \n",
    "    if(fitted_model != None):\n",
    "        if do_probabilities:\n",
    "            pred = fitted_model.predict_proba(X_test_data)\n",
    "        else:\n",
    "            pred = fitted_model.predict(X_test_data)\n",
    "            \n",
    "        return fitted_model, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dso7yIy6wPhb"
   },
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "cuKwVKapwSgb",
    "outputId": "034164ff-9653-4048-b68b-4a0e2cf1ba5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626373626373627\n",
      "{'n_estimators': 1000, 'max_leaf_nodes': 100, 'max_depth': 25}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'max_depth': [15,20,25],\n",
    "    'max_leaf_nodes': [50, 100, 200]\n",
    "}\n",
    "\n",
    "model, pred = search_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy',\n",
    "                                 search_mode = 'RandomizedSearchCV', n_iterations = 15)\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hyperparameters_Tuning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
